{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigating Grid world with Monte Carlo Sampling Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, I implement a grid world policy evaluation and update using Monte Carlo sampling.\n",
    "The grid world is a 3 x 4 world, with a wall. There is a win state and a lose state.\n",
    "Each run begins in cell [2,0], and the goal is to reach the WIN state, and avoid the LOSE state.\n",
    "\n",
    "The agent may move left, right, up or down. If the agent hits a wall, they will return to the state they were just at.\n",
    "The reward for each step taken is 0. A reward is only provided in the WIN or LOSE state.\n",
    "\n",
    "<img src=\"misc/simple_Gridworld.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code applied Monte Carlo sampling to determine the best state action pairs to go from the start to the win state.\n",
    "\n",
    "Initially, the policy is random. After each episode, the policy is updated to select the one which returns the highest action value from a given state. There is however a $\\epsilon$ = 0.1 probability of selecting a random action (to allow exploration).\n",
    "\n",
    "Below is the pseudocode for the method. Here the method to calculate the gain for each state is also demonstrated (by working backwards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"misc/MonteCarlo_GridWorld_Psuedocode.PNG\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary of the state action pairs for all possible states is maintained and referred to when it is time to update the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "How Monte Carlo policy evaluation and improvement works is by completing an episode following the initial policy. In the case of my code, this policy is random in each direction. During the episode, we keep track of all the actions taken from each state that was visited. In addition, we keep track of the reward recieved from each transition into a new state.\n",
    "\n",
    "At the end of an epsiode, we work backwards to calculate the gain from each state action pair.\n",
    "\n",
    "This process is repeated for multiple episodes, and the gain for each state action pair is averaged.\n",
    "\n",
    "After each episode, the updated state-action pairs are used to update the policy. The policy for the next iteration is one which, for a given state, takes an action which maximises the state-action pair value.\n",
    "\n",
    "To ensure exploration, we have an $\\eta$ probability of not following the polcy and instead following a random policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The results of my code return the following directions for each of the states.\n",
    "This was the results from a random run of 1000 episodes.\n",
    "The decay $\\gamma$ was 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[2, 3]', 'left']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['[0, 0]', 'right']\n",
    "['[0, 1]', 'right']\n",
    "['[0, 2]', 'right']\n",
    "['[1, 0]', 'up']\n",
    "['[1, 2]', 'up']\n",
    "['[2, 0]', 'up']\n",
    "['[2, 1]', 'left']\n",
    "['[2, 2]', 'down']\n",
    "['[2, 3]', 'left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"misc/simple_Gridworld_1000_iterations.png\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10,000 episodes, the policy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[2, 3]', 'left']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['[0, 0]', 'right']\n",
    "['[0, 1]', 'right']\n",
    "['[0, 2]', 'right']\n",
    "['[1, 0]', 'up']\n",
    "['[1, 2]', 'up']\n",
    "['[2, 0]', 'up']\n",
    "['[2, 1]', 'left']\n",
    "['[2, 2]', 'left']\n",
    "['[2, 3]', 'left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"misc/simple_Gridworld_10000_iterations.PNG\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
