{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigating Grid world with DYNA-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, I implement a the DYNA-Q algorithm on both a simple and complex grid world.\n",
    "\n",
    "Unlike the other methods of Monte Carlo, SARA and Q-Learning, DYNA-Q implements Q-Learning but with a planning stage between steps.\n",
    "This makes the most of the observations made so far.\n",
    "\n",
    "In planning the algorithm iterates through the points observed to update the action-values.\n",
    "\n",
    "### DYNA-Q Action Value Update\n",
    "Is still the same as the Q-Learning algorithm\n",
    "<img src=\"misc/QLearning_ActionValueUpdate.PNG\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID WORLDS\n",
    "\n",
    "There are two grid worlds that DYNA-Q is applied for:\n",
    "\n",
    "The simple grid world (same as that in the Monte Carlo, Q-Learning and SARSA).\n",
    "Again we implement the 3 x 4 grid world. There is a win state and a lose state and a wall.\n",
    "Each run begins in cell [2,0], and the goal is to reach the WIN state, and avoid the LOSE state.\n",
    "\n",
    "<img src=\"misc/simple_Gridworld.png\" width=\"400\"/>\n",
    "\n",
    "And a complex grid world where the agent starts in cell [2,0] and the win state is at [0,8].\n",
    "There are more walls in this complex world\n",
    "\n",
    "<img src=\"misc/Complex_Gridworld.PNG\" width=\"400\"/>\n",
    "\n",
    "The actions of the agents are the same as before:\n",
    "The agent may move left, right, up or down. If the agent hits a wall, they will return to the state they were just at.\n",
    "The reward for each step taken is 0. A reward is only provided in the WIN or LOSE state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the policy is random. After each episode, the policy is updated to select the one which returns the highest action value from a given state. There is however a $\\epsilon$ = 0.1 probability of selecting a random action (to allow exploration).\n",
    "\n",
    "Below is the pseudocode for the method. \n",
    "Policy improvement is via the $\\epsilon$-soft method shown in the Monte-Carlo article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"misc/TabularDYNAQ_Psuedocode.PNG\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary of the state action pairs for all possible states is maintained and referred to when it is time to update the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "How DYNA-Q works is, when initialised (a) it will first select action to take based on the policy (b).\n",
    "After the action is taken, it will observe the reward and the next state (c).\n",
    "\n",
    "This information is used to apply a Q-Learning update to $Q(S,A)$ (d).\n",
    "\n",
    "This transition information is stored in a dictionary that models the environment (we assume a deterministic environment).\n",
    "So the information stored is that, if I start in state $S$ and take action $A$, I will end up in state $S'$ with reward $R$.\n",
    "\n",
    "After this move in the 'Real world', the algorithm takes the model information and uses it to iterate and refine the action-values, in a planning stage (this is like what we do in our heads). In each step of planning, the agent randomly goes through the state and associated actions it has seen in the past, and using the model, update the action-values.The planning stage is repeated $n$ (f).\n",
    "\n",
    "If the number of planning steps is set to 0, the DYNA-Q code is just normal Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The results of my code return the following directions for each of the states.\n",
    "The decay $\\gamma$ was 0.9\n",
    "\n",
    "In these experiments, varying numbers of planning steps were tests.\n",
    "\n",
    "50 planning steps\n",
    "\n",
    "5 planning steps\n",
    "\n",
    "0 planning steps (normal Q-Learning)\n",
    "\n",
    "It can be observed that with 50 planning steps, the algoirithm is able to hone into an optimal path after a few episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the number of steps required to reach the WIN STATE over the number of trials, the following is observed:\n",
    "\n",
    "### Simple Grid World\n",
    "<img src=\"misc/DYNA_Q_Simple_performance.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Grid World\n",
    "\n",
    "<img src=\"misc/DYNA_Q_Complex_performance.png\" width=\"400\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
