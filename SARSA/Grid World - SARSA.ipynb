{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigating Grid world with SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, I implement a grid world policy evaluation and update using the SARSA algorithm.\n",
    "\n",
    "Unlike Monte Carlo methods which require you to finish an episode before updating state/action values, using Temporal Difference methods allows you to update these as you go.\n",
    "Ulitimately allowing you to converge to state values faster.\n",
    "\n",
    "In order to do policy improvement as well, not just policy evaluation, determining the action values is important $Q(S_t,A_t)$.\n",
    "In Monte-Carlo, these are updated by the following, where the value is updated after each episode (when the expected gains are calculated backwards).\n",
    "\n",
    "### Monte Carlo Action Value Update\n",
    "<img src=\"misc/MC_ActionValueUpdate.PNG\" width=\"400\"/>\n",
    "\n",
    "In SARSA, you don't need to wait until the end of the episode. Rather you are able to update the action-value after obtaining the reward, and selecting the next state-action pair.\n",
    "It is of the form:\n",
    "\n",
    "### SARSA Action Value Update\n",
    "<img src=\"misc/SARSA_ActionValueUpdate.PNG\" width=\"400\"/>\n",
    "\n",
    "This sequence is represented in the name - SARSA ($S_t$,$A_t$,$R_{t+1}$,$S_{t+1}$,$A_{t+1}$)\n",
    "\n",
    "<img src=\"misc/SARSA.PNG\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we implement the 3 x 4 grid world. There is a win state and a lose state and a wall.\n",
    "Each run begins in cell [2,0], and the goal is to reach the WIN state, and avoid the LOSE state.\n",
    "\n",
    "The agent may move left, right, up or down. If the agent hits a wall, they will return to the state they were just at.\n",
    "The reward for each step taken is 0. A reward is only provided in the WIN or LOSE state.\n",
    "\n",
    "<img src=\"misc/simple_Gridworld.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code applied Monte Carlo sampling to determine the best state action pairs to go from the start to the win state.\n",
    "\n",
    "Initially, the policy is random. After each episode, the policy is updated to select the one which returns the highest action value from a given state. There is however a $\\epsilon$ = 0.1 probability of selecting a random action (to allow exploration).\n",
    "\n",
    "Below is the pseudocode for the method. \n",
    "Policy improvement is via the $\\epsilon$-soft method shown in the Monte-Carlo article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"misc/SARSA_Psuedocode.PNG\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary of the state action pairs for all possible states is maintained and referred to when it is time to update the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "How SARSA policy evaluation and improvement works is by:\n",
    "When in a given state, refer to the current policy $\\pi$ to select the best action for the state.\n",
    "Take this action and observe the reward $R$ and next state $S'$.\n",
    "Then use the the curreny policy to select the best action $A'$ for this new state.\n",
    "\n",
    "These can be used to now update the action-value for $Q(S,A)$.\n",
    "\n",
    "After a certain period of time - in my code, after each epsiode, the policy is updated using the $\\epsilon$-soft method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The results of my code return the following directions for each of the states.\n",
    "The decay $\\gamma$ was 0.9\n",
    "This was from 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[2, 3]', 'left']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['[0, 0]', 'up']\n",
    "['[0, 1]', 'right']\n",
    "['[0, 2]', 'right']\n",
    "['[1, 0]', 'down']\n",
    "['[1, 2]', 'up']\n",
    "['[2, 0]', 'right']\n",
    "['[2, 1]', 'right']\n",
    "['[2, 2]', 'up']\n",
    "['[2, 3]', 'left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the number of steps required to reach the WIN STATE over the number of trials, the following is observed:\n",
    "\n",
    "<img src=\"misc/SARSA_performance.png\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
