{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation with Gradient Monte Carlo - Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous codes, the number of states in an environment have been manageable, and state value calculations have been possible for each state discriminantly (so tuning for each state independently of influencing other states).\n",
    "\n",
    "But in more complex environments, the number of states might be very large or even infinite (continuous state spaces).\n",
    "\n",
    "For this, function approximation is needed for determining the state values.\n",
    "\n",
    "The general form of linear state-value function approximation is:\n",
    "\n",
    "<img src=\"misc/Linear_Function_Approximation.PNG\" width=\"300\"/>\n",
    "\n",
    "where $x$ is the features used to represent a state, and $w$ are the weights. The multiplication of these two vectors results in a scalar value to be an approximation of the state value for that state. The performance of the approximation is measured by the mean squared error value. We aim to minimise this value:\n",
    "\n",
    "<img src=\"misc/MSE.PNG\" width=\"300\"/>\n",
    "\n",
    "The key component in function approximation is to correctly tune the $w$ weights. This is done via gradient descent.\n",
    "Applying the derivative, a weight update can be calculated as:\n",
    "\n",
    "<img src=\"misc/weight_update_true.PNG\" width=\"300\"/>\n",
    "\n",
    "In practise, $v_{\\pi}(S_t)$ the true value of the state for a given policy is unknown and as such needs to be estimated through the epsiodes. So the formula is adapted such that this estimate is given as $U_t$.\n",
    "\n",
    "<img src=\"misc/weight_update.PNG\" width=\"300\"/>\n",
    "\n",
    "Additionally, for linear state-value function approximations, $\\nabla \\hat{v}(S_t,w_t)$ is $x(S_t)$.\n",
    "\n",
    "<img src=\"misc/weight_update_linear.PNG\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Monte-Carlo\n",
    "\n",
    "For Gradient Monte-Carlo methods, $U_t$ is $G_t$.\n",
    "As such the pseudocode for policy evaluation for Gradient Monte Carlo is given as:\n",
    "\n",
    "<img src=\"misc/GradientMC_Psuedocode.PNG\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "We applied gradient Monte-Carlo on a 1000-state Random Walk.\n",
    "This experiment starts at state 500, and the agent has a random left or right action policy.\n",
    "\n",
    "The agent policy also returns a value between 1 and 100 to represent the number of steps it jumps in the direction selected.\n",
    "The goal is to reach the terminal states at either 0, or 1001. \n",
    "At 0 a reward of -1 is returned, and at 1001 a reward of 1 is returned.\n",
    "\n",
    "If the number of hops takes the agent off the board, it lands in the terminal state.\n",
    "Eg. if agent is currently in state 20, and policy returns a left move with a hop size of 50, then the agent jumps to state 0.\n",
    "\n",
    "We apply state aggregation to the method, where states are aggregated into groups of 100.\n",
    "\n",
    "The following results are observed after 10,000 epsiodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"misc/Gradient_MC.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
